{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%stop_session\n",
				"#%additional_python_modules pyiceberg==0.3.0,pyspark==3.2.0"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%idle_timeout 11520\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"%region us-east-1\n",
				"%connections connection1,connection2\n",
				"%max_concurrent_runs 20\n",
				"%%configure\n",
				"{\n",
				"    \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog.warehouse= --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\",\n",
				"    \"--datalake-formats\": \"iceberg\",\n",
				"    \"--JOB_NAME\":\"\",\n",
				"    \"--data\":\"CURRENT_DATE\",\n",
				"    \"--intervalo\":\"0\",\n",
				"    \"--catalog\":\"glue_catalog\",\n",
				"    \"--database\":\"db_bronze\",\n",
				"    \"--table\":\"\",\n",
				"    \"--connection\":\"\",\n",
				"    \"--query\":\"\",\n",
				"    \"--match_id\":\"id\",\n",
				"    \"--metodo\":\"drop-insert\",\n",
				"    \"--partition\": \"true\",\n",
				"    \"--enable-metrics\": \"true\",\n",
				"    \"--enable-continuous-cloudwatch-log\": \"true\",\n",
				"    \"--enable-spark-ui\": \"true\",\n",
				"    \"--spark-event-logs-path\":\"\"\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql import SparkSession\n",
				"from pyspark.sql.functions import col, when, date_add, date_format\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.sql.functions import col, when, lit\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from datetime import datetime, timedelta, date\n",
				"import pandas as pd\n",
				"\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME','data','intervalo','catalog','database','table','connection','query','match_id','metodo','partition'])\n",
				"job.init(args['JOB_NAME'], args)\n",
				"\n",
				"par_date = args['data']\n",
				"par_intervalo = int(args['intervalo'])\n",
				"catalog_name = args['catalog']\n",
				"database_name = args['database']\n",
				"table_name = args['table']\n",
				"par_connection = args['connection']\n",
				"par_query = args['query']\n",
				"match_id = args['match_id']\n",
				"metodo = args['metodo']\n",
				"partition = args['partition']\n",
				"\n",
				"\n",
				"if par_date == 'CURRENT_DATE':\n",
				"    par_date = date.today()\n",
				"    par_date = par_date.strftime(\"%Y-%m-%d\")\n",
				"\n",
				"def substituir_parametros(query,parametros):\n",
				"    for chave, valor in parametros.items():\n",
				"        query = query.replace(f\"{{{chave}}}\", valor)\n",
				"    return query\n",
				"\n",
				"def get_driver (driver_name):\n",
				"    drivers = {\n",
				"        \"mysql\": \"com.mysql.cj.jdbc.Driver\",\n",
				"        \"postgresql\": \"org.postgresql.Driver\",\n",
				"        \"sqlserver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
				"    }\n",
				"    return drivers[driver_name]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"connection_options = glueContext.extract_jdbc_conf(par_connection)\n",
				"source_url = connection_options.get(\"fullUrl\")\n",
				"source_user = connection_options.get(\"user\")\n",
				"source_password = connection_options.get(\"password\")\n",
				"source_driver = get_driver (connection_options.get(\"vendor\"))\n",
				"\n",
				"temp_table_name = f\"tmp_{table_name}\"\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"if metodo == 'merge-incremental':\n",
				"    \n",
				"    # Criar DataFrame com datas de referência\n",
				"    date_final = datetime.strptime(par_date, \"%Y-%m-%d\")\n",
				"    date_inicial = date_final - timedelta(days=par_intervalo)\n",
				"    lt_periodo = pd.date_range(start=date_inicial, end=date_final).to_pydatetime().tolist()\n",
				"    lt_periodo.sort(reverse=False)\n",
				"    # Criar um DataFrame Spark a partir da lista de datas\n",
				"    df_datas = spark.createDataFrame([(d,) for d in lt_periodo], [\"data_referencia\"])\n",
				"    # Converter coluna de data para formato de data do Spark\n",
				"    df_datas = df_datas.withColumn(\"data_referencia\", col(\"data_referencia\").cast(\"date\"))\n",
				"\n",
				"    for row in df_datas.collect():\n",
				"        data_referencia = row.data_referencia\n",
				"        parametros = {\n",
				"        \"par_referencia\": data_referencia.strftime(\"%Y-%m-%d\"),\n",
				"        \"par_data_inicial_timestamp\": data_referencia.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
				"        \"par_data_final_timestamp\": data_referencia.strftime(\"%Y-%m-%d 23:59:59\"),\n",
				"        \"par_data_inicial_extra\": (data_referencia - timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
				"        \"par_data_final_extra\": (data_referencia + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
				"        \"par_data_inicial_extra_timestamp\": (data_referencia - timedelta(days=1)).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
				"        \"par_data_final_extra_timestamp\": (data_referencia + timedelta(days=1)).strftime(\"%Y-%m-%d 23:59:59\")\n",
				"        }\n",
				"        print(f\"Executando tabela {catalog_name}.{database_name}.{table_name} para a data de {data_referencia}\")\n",
				"\n",
				"        #Carregar DataFrame##################################################################################################################################\n",
				"\n",
				"        par_query_substituida = substituir_parametros(par_query,parametros)\n",
				"        print(f\"Executando Consulta = {par_query_substituida}\")\n",
				"        df = spark.read \\\n",
				"        .format(\"jdbc\") \\\n",
				"        .option(\"url\", source_url) \\\n",
				"        .option(\"dbtable\", par_query_substituida) \\\n",
				"        .option(\"user\", source_user) \\\n",
				"        .option(\"password\", source_password) \\\n",
				"        .option(\"driver\", source_driver) \\\n",
				"        .load()\n",
				"        df.createOrReplaceTempView(temp_table_name)\n",
				"\n",
				"        #####################################################################################################################################################\n",
				"\n",
				"        #Criação de base de dados caso nao exista\n",
				"        query_database = f\"\"\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\"\"\"\n",
				"        spark.sql(query_database)\n",
				"        print(f\"Criando DataBase caso nao exista = {query_database}\")\n",
				"\n",
				"        if partition == True:\n",
				"            #Criação de tabela caso nao exista\n",
				"            query_table = f\"\"\"\n",
				"            CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"            USING iceberg \n",
				"            PARTITIONED BY (days(partition_date)) \n",
				"            TBLPROPERTIES (\"format-version\"=\"2\")\n",
				"            AS SELECT * FROM {temp_table_name} ORDER BY partition_date,{match_id}\n",
				"            \"\"\"\n",
				"        else:\n",
				"            #Criação de tabela caso nao exista\n",
				"            query_table = f\"\"\"\n",
				"            CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"            USING iceberg \n",
				"            TBLPROPERTIES (\"format-version\"=\"2\")\n",
				"            AS SELECT * FROM {temp_table_name}\n",
				"            \"\"\"\n",
				"    \n",
				"        spark.sql(query_table)  \n",
				"        print(f\"Criando Tabela caso nao exista = {catalog_name}.{database_name}.{table_name}\")\n",
				"  \n",
				"        #Inclusão de novas colunas na tabela existente\n",
				"        esquema_tabela = spark.table(f\"{catalog_name}.{database_name}.{table_name}\").schema\n",
				"        colunas_dataframe = df.columns\n",
				"        colunas_tabela = [campo.name for campo in esquema_tabela.fields if campo != {match_id}]\n",
				"        novas_colunas = list(set(colunas_dataframe) - set(colunas_tabela))\n",
				"\n",
				"        for coluna in novas_colunas:\n",
				"            print(f\"Adicionando nova coluna à tabela Iceberg: {coluna}\")\n",
				"            tipo_de_dado = df.schema[coluna].dataType\n",
				"            sql_coluna = f\"\"\"\n",
				"            ALTER TABLE {catalog_name}.{database_name}.{table_name}\n",
				"            ADD COLUMNS ({coluna} string)\n",
				"            \"\"\"\n",
				"            spark.sql(sql_coluna)\n",
				"\n",
				"        #Preparação dinamica do merge\n",
				"        colunas = [coluna for coluna in df.columns if coluna != match_id]\n",
				"        condicao_select = \" \".join([f\"b.{c} as {c},\" for c in colunas])\n",
				"        condicao_where = \" AND \".join([f\"a.{c} = b.{c}\" for c in colunas])\n",
				"        par_referencia = parametros['par_referencia']\n",
				"\n",
				"        query_merge = f\"\"\"\n",
				"        WITH changes AS\n",
				"        (SELECT\n",
				"        COALESCE(b.{match_id}, a.{match_id}) AS {match_id}, {condicao_select}\n",
				"        CASE WHEN b.{match_id} IS NULL THEN 'D' WHEN a.{match_id} IS NULL THEN 'I' ELSE 'U' END as cdc\n",
				"        FROM {catalog_name}.{database_name}.{table_name} a\n",
				"        FULL OUTER JOIN {temp_table_name} b ON a.{match_id} = b.{match_id}\n",
				"        WHERE b.partition_date = '{par_referencia}'\n",
				"        AND NOT coalesce(({condicao_where}), false))\n",
				"\n",
				"        MERGE INTO {catalog_name}.{database_name}.{table_name}\n",
				"        USING changes\n",
				"        ON {catalog_name}.{database_name}.{table_name}.{match_id} = changes.{match_id}\n",
				"        WHEN MATCHED AND changes.cdc = 'D' THEN DELETE\n",
				"        WHEN MATCHED AND changes.cdc = 'U' THEN UPDATE SET *\n",
				"        WHEN NOT MATCHED THEN INSERT *\n",
				"        \"\"\"\n",
				"        print(f\"Fazendo Merge da tabela tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"        spark.sql(query_merge)\n",
				"        print(query_merge)\n",
				"    \n",
				"elif metodo == 'drop-insert':    \n",
				"    \n",
				"    #Carregar DataFrame##################################################################################################################################\n",
				"\n",
				"    print(f\"Executando Consulta = {par_query}\")\n",
				"    df = spark.read \\\n",
				"    .format(\"jdbc\") \\\n",
				"    .option(\"url\", source_url) \\\n",
				"    .option(\"dbtable\", par_query) \\\n",
				"    .option(\"user\", source_user) \\\n",
				"    .option(\"password\", source_password) \\\n",
				"    .option(\"driver\", source_driver) \\\n",
				"    .load()\n",
				"    df.createOrReplaceTempView(temp_table_name)\n",
				"\n",
				"    #####################################################################################################################################################\n",
				"\n",
				"    #Criação de base de dados caso nao exista\n",
				"    query_database = f\"\"\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\"\"\"\n",
				"    spark.sql(query_database)\n",
				"    print(f\"Criando DataBase caso nao exista = {query_database}\")\n",
				"    \n",
				"    if partition == True:\n",
				"        #Criação de tabela caso nao exista\n",
				"        query_table = f\"\"\"\n",
				"        CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"        USING iceberg \n",
				"        PARTITIONED BY (days(partition_date)) \n",
				"        TBLPROPERTIES (\"format-version\"=\"2\")\n",
				"        AS SELECT * FROM {temp_table_name} ORDER BY partition_date,{match_id}\n",
				"        \"\"\"\n",
				"    else:\n",
				"        #Criação de tabela caso nao exista\n",
				"        query_table = f\"\"\"\n",
				"        CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"        USING iceberg \n",
				"        TBLPROPERTIES (\"format-version\"=\"2\")\n",
				"        AS SELECT * FROM {temp_table_name}\n",
				"        \"\"\"\n",
				"\n",
				"    spark.sql(query_table)  \n",
				"    print(f\"Criando Tabela caso nao exista = {catalog_name}.{database_name}.{table_name}\")\n",
				"\n",
				"    #Inclusão de novas colunas na tabela existente\n",
				"    esquema_tabela = spark.table(f\"{catalog_name}.{database_name}.{table_name}\").schema\n",
				"    colunas_dataframe = df.columns\n",
				"    colunas_tabela = [campo.name for campo in esquema_tabela.fields if campo != {match_id}]\n",
				"    novas_colunas = list(set(colunas_dataframe) - set(colunas_tabela))\n",
				"\n",
				"    for coluna in novas_colunas:\n",
				"        print(f\"Adicionando nova coluna à tabela Iceberg: {coluna}\")\n",
				"        tipo_de_dado = df.schema[coluna].dataType\n",
				"        sql_coluna = f\"\"\"\n",
				"        ALTER TABLE {catalog_name}.{database_name}.{table_name}\n",
				"        ADD COLUMNS ({coluna} string)\n",
				"        \"\"\"\n",
				"        spark.sql(sql_coluna)\n",
				"\n",
				"    query_truncate= f\"\"\"TRUNCATE TABLE {catalog_name}.{database_name}.{table_name}\"\"\"\n",
				"    print(f\"Truncando tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"    spark.sql(query_truncate)\n",
				"    \n",
				"    if partition == True:\n",
				"        query_insert = f\"\"\"\n",
				"        INSERT INTO {catalog_name}.{database_name}.{table_name} \n",
				"        SELECT * FROM {temp_table_name} ORDER BY partition_date,{match_id}\"\"\"\n",
				"    else:\n",
				"        query_insert = f\"\"\"\n",
				"        INSERT INTO {catalog_name}.{database_name}.{table_name} \n",
				"        SELECT * FROM {temp_table_name}\"\"\"\n",
				"    \n",
				"    spark.sql(query_insert)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"job.commit()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%stop_session"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}

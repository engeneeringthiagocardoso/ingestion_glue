{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%stop_session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"%%configure\n",
				"{\n",
				"    \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog.warehouse= --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\",\n",
				"    \"--datalake-formats\": \"iceberg\",\n",
				"    \"--additional-python-modules\": \"requests_ntlm,openpyxl==3.0.10\",\n",
				"    \"--JOB_NAME\": \"sharepoint-sap\",\n",
				"    \"--path\":\"\",\n",
				"    \"--config\":\"\",\n",
				"    \"--enable-metrics\": \"true\",\n",
				"    \"--enable-continuous-cloudwatch-log\": \"true\",\n",
				"    \"--enable-spark-ui\": \"true\",\n",
				"    \"--spark-event-logs-path\":\"\"\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 72,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"import requests\n",
				"from requests_ntlm import HttpNtlmAuth\n",
				"import boto3\n",
				"from botocore.exceptions import NoCredentialsError, PartialCredentialsError, ClientError\n",
				"from urllib.parse import urlparse\n",
				"import os\n",
				"from datetime import date,datetime\n",
				"from dateutil.relativedelta import relativedelta\n",
				"import pandas as pd\n",
				"import s3fs\n",
				"import json\n",
				"import ast\n",
				"\n",
				"fs = s3fs.S3FileSystem(anon=False)\n",
				"pd.set_option('display.max_columns', None)  \n",
				"\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME','config'])\n",
				"job.init(args['JOB_NAME'], args)\n",
				"par_config = args['config']\n",
				"    \n",
				"def gerar_lista_meses(data, intervalo):\n",
				"    # Converter a string de data para objeto datetime\n",
				"    data_atual = datetime.strptime(data, \"%Y-%m-%d\")\n",
				"    \n",
				"    # Criar uma lista para armazenar os meses\n",
				"    lista_meses = []\n",
				"    \n",
				"    # Iterar mês a mês, retroagindo\n",
				"    for i in range(intervalo + 1):\n",
				"        # Calcular o mês para cada iteração\n",
				"        mes = data_atual - relativedelta(months=i)\n",
				"        \n",
				"        # Adicionar o mês à lista no formato YYYYMM\n",
				"        lista_meses.append(mes.strftime(\"%Y%m\"))\n",
				"    return lista_meses\n",
				"\n",
				"def read_json_from_s3(s3_uri):\n",
				"    # Parse o S3 URI\n",
				"    parsed_uri = urlparse(s3_uri)\n",
				"    bucket = parsed_uri.netloc\n",
				"    key = parsed_uri.path.lstrip('/')\n",
				"\n",
				"    # Crie um cliente S3\n",
				"    s3 = boto3.client('s3')\n",
				"\n",
				"    try:\n",
				"        # Obtenha o objeto do S3\n",
				"        response = s3.get_object(Bucket=bucket, Key=key)\n",
				"        \n",
				"        # Leia o conteúdo e decodifique\n",
				"        content = response['Body'].read().decode('utf-8')\n",
				"\n",
				"        # Parse o JSON\n",
				"        return json.loads(content)\n",
				"    except Exception as e:\n",
				"        print(f\"Erro ao ler o arquivo JSON do S3: {str(e)}\")\n",
				"        raise\n",
				"\n",
				"def arquivo_existe_no_s3(s3_uri):\n",
				"    \"\"\"\n",
				"    Verifica se um arquivo existe no Amazon S3 usando o S3 URI.\n",
				"\n",
				"    :param s3_uri: O URI do S3 no formato 's3://bucket-name/key/to/object'\n",
				"    :return: True se o arquivo existe, False caso contrário\n",
				"    \"\"\"\n",
				"    # Parse o S3 URI\n",
				"    parsed_uri = urlparse(s3_uri)\n",
				"    if parsed_uri.scheme != 's3':\n",
				"        raise ValueError(\"URI inválido. Deve começar com 's3://'\")\n",
				"    \n",
				"    bucket = parsed_uri.netloc\n",
				"    key = parsed_uri.path.lstrip('/')\n",
				"\n",
				"    # Inicializa o cliente S3\n",
				"    s3_client = boto3.client('s3')\n",
				"\n",
				"    try:\n",
				"        # Tenta fazer uma chamada head_object para o arquivo\n",
				"        s3_client.head_object(Bucket=bucket, Key=key)\n",
				"        return True\n",
				"    except ClientError as e:\n",
				"        if e.response['Error']['Code'] == '404':\n",
				"            # O arquivo não existe\n",
				"            return False\n",
				"        else:\n",
				"            # Outro erro ocorreu\n",
				"            raise \n",
				"        \n",
				"\n",
				"def process_dataframe(df, schema_map):\n",
				"    \n",
				"    if schema_map is None:\n",
				"        return df\n",
				"\n",
				"    for column, rules in schema_map.items():\n",
				"        if rules['remove'] == \"True\":\n",
				"            df.drop(columns=[column], inplace=True, errors='ignore')\n",
				"        else:\n",
				"            if rules['rename'] != \"-\":\n",
				"                df.rename(columns={column: rules['rename']}, inplace=True)\n",
				"            if rules['type'] != \"-\":\n",
				"                if rules['type'] == \"int\":\n",
				"                    df[rules['rename']] = pd.to_numeric(df[rules['rename']], errors='coerce').fillna(0).astype(int)\n",
				"                elif rules['type'] == \"float\":\n",
				"                    df[rules['rename']] = pd.to_numeric(df[rules['rename']], errors='coerce').fillna(0.0).astype(float)\n",
				"                elif rules['type'] == \"str\":\n",
				"                    df[rules['rename']] = df[rules['rename']].astype(str).replace('nan', '').fillna('')\n",
				"                elif rules['type'] == \"date\":\n",
				"                    df[rules['rename']] = pd.to_datetime(df[rules['rename']], errors='coerce').dt.date\n",
				"                    df[rules['rename']].fillna(pd.Timestamp(0).date(), inplace=True)\n",
				"                elif rules['type'] == \"timestamp\":\n",
				"                    df[rules['rename']] = pd.to_datetime(df[rules['rename']], errors='coerce')\n",
				"                    df[rules['rename']].fillna(pd.Timestamp(0), inplace=True)\n",
				"                elif rules['type'] == \"timestamp_excel\":\n",
				"                    df[rules['rename']] = pd.to_datetime('1899-12-30') + pd.to_timedelta(df[rules['rename']], 'D')\n",
				"                    df[rules['rename']].fillna(pd.Timestamp(0), inplace=True)\n",
				"\n",
				"    return df\n",
				"\n",
				"def processa_lake(metodo,snapshot,catalog_name,database_name,table_name,match_id,mes,df):\n",
				"    \n",
				"    temp_table_name = f\"tmp_{table_name}\"\n",
				"    spark_df = spark.createDataFrame(df)\n",
				"    spark_df.createOrReplaceTempView(temp_table_name)\n",
				"    #Criação de base de dados caso nao exista\n",
				"    query_database = f\"\"\"CREATE DATABASE IF NOT EXISTS {catalog_name}.{database_name}\"\"\"\n",
				"    spark.sql(query_database)\n",
				"    print(f\"Criando DataBase caso nao exista = {query_database}\")\n",
				"\n",
				"    #Criação de tabela caso nao exista\n",
				"    query_table = f\"\"\"\n",
				"    CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.{table_name}\n",
				"    USING iceberg \n",
				"    TBLPROPERTIES (\"format-version\"=\"2\")\n",
				"    AS SELECT * FROM {temp_table_name}\"\"\"\n",
				"    spark.sql(query_table)  \n",
				"    print(f\"Criando Tabela caso nao exista = {catalog_name}.{database_name}.{table_name}\")\n",
				"\n",
				"    if snapshot == False:\n",
				"        query_snapshot = f\"\"\"ALTER TABLE {catalog_name}.{database_name}.{table_name} SET TBLPROPERTIES ('history.expire.max-snapshots' = '10')\"\"\"\n",
				"        spark.sql(query_snapshot)\n",
				"        print(f\"Removendo Snapshot tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"\n",
				"    if metodo == 'merge-full':\n",
				"\n",
				"        colunas = [coluna for coluna in df.columns if coluna != match_id]\n",
				"        condicao_select = \" \".join([f\"b.{c} as {c},\" for c in colunas])\n",
				"        condicao_where = \" AND \".join([f\"a.{c} = b.{c}\" for c in colunas])\n",
				"        \n",
				"        query_merge = f\"\"\"\n",
				"        WITH changes AS\n",
				"        (SELECT\n",
				"        COALESCE(b.{match_id}, a.{match_id}) AS {match_id}, {condicao_select}\n",
				"        CASE WHEN b.{match_id} IS NULL THEN 'D' WHEN a.{match_id} IS NULL THEN 'I' ELSE 'U' END as cdc\n",
				"        FROM {catalog_name}.{database_name}.{table_name} a\n",
				"        FULL OUTER JOIN {temp_table_name} b ON a.{match_id} = b.{match_id}\n",
				"        WHERE NOT coalesce(({condicao_where}), false))\n",
				"        MERGE INTO {catalog_name}.{database_name}.{table_name}\n",
				"        USING changes\n",
				"        ON {catalog_name}.{database_name}.{table_name}.{match_id} = changes.{match_id}\n",
				"        WHEN MATCHED AND changes.cdc = 'D' THEN DELETE\n",
				"        WHEN MATCHED AND changes.cdc = 'U' THEN UPDATE SET *\n",
				"        WHEN NOT MATCHED THEN INSERT *\n",
				"        \"\"\"\n",
				"        print(f\"Fazendo Merge da tabela tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"        spark.sql(query_merge)\n",
				"        return True\n",
				"    \n",
				"    elif metodo == 'drop-insert':\n",
				"        \n",
				"        query_truncate= f\"\"\"TRUNCATE TABLE {catalog_name}.{database_name}.{table_name}\"\"\"\n",
				"        spark.sql(query_truncate)\n",
				"        print(f\"Truncando tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"        query_insert = f\"\"\"INSERT INTO {catalog_name}.{database_name}.{table_name} SELECT * FROM {temp_table_name}\"\"\"\n",
				"        spark.sql(query_insert)\n",
				"        print(f\"Insert tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"        return True\n",
				"    \n",
				"    elif metodo == 'delete-insert-mes':\n",
				"\n",
				"        query_delete= f\"\"\"DELETE FROM {catalog_name}.{database_name}.{table_name} WHERE {match_id} = {mes}\"\"\"\n",
				"        spark.sql(query_delete)\n",
				"        print(f\"DELETANDO tabela {catalog_name}.{database_name}.{table_name} WHERE {match_id} = {mes}\")\n",
				"        query_insert = f\"\"\"INSERT INTO {catalog_name}.{database_name}.{table_name} SELECT * FROM {temp_table_name}\"\"\"\n",
				"        spark.sql(query_insert)\n",
				"        print(f\"Insert tabela {catalog_name}.{database_name}.{table_name}\")\n",
				"        return True\n",
				"\n",
				"    else:\n",
				"        return False\n",
				"    \n",
				"    return True\n",
				"\n",
				"def get_tipo_by_valor(valor, data):\n",
				"    \n",
				"    for item in data:\n",
				"        if item['valor'] == valor:\n",
				"            return item['tipo']\n",
				"    return None\n",
				"\n",
				"def extrair_tipo(bigint, data):\n",
				"\n",
				"    primeiro_digito = int(str(bigint)[0])\n",
				"    return get_tipo_by_valor(primeiro_digito, data)\n",
				"\n",
				"def transformacao_dataframe(df_trans,function):\n",
				"    \n",
				"    if function == 'default':\n",
				"        \n",
				"        return df_trans\n",
				"    \n",
				"    elif function == 'transformacao_sap_balancete':\n",
				"\n",
				"        filtro_tipo = (df_trans['Unnamed: 2'] != 2900 ) & (df_trans['Unnamed: 4'] != '')\n",
				"        df_trans_tipo = df_trans.loc[filtro_tipo]\n",
				"        df_trans_tipo['mapa'] = df_trans_tipo['Unnamed: 4'].apply(lambda x: x if isinstance(x, str) and len(x.split(' ')[0]) == 1 else None)\n",
				"        df_trans_tipo = df_trans_tipo.dropna(subset=['mapa'])\n",
				"        coluna_remove = [\"Unnamed: 0\", \"Unnamed: 1\", \"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\",\"Unnamed: 5\", \"Unnamed: 6\",\"Unnamed: 7\",\"Unnamed: 8\", \"Unnamed: 9\",\"Unnamed: 10\",\"Unnamed: 11\",\"Unnamed: 12\",\"Unnamed: 13\", \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\", \"Unnamed: 17\"]\n",
				"        df_trans_tipo.drop(columns=coluna_remove, inplace=True)\n",
				"        df_trans_tipo[['valor', 'tipo']] = df_trans_tipo['mapa'].str.split(' ', 1, expand=True)\n",
				"        df_trans_tipo['valor'] = df_trans_tipo['valor'].astype(int)\n",
				"        df_trans_tipo.drop(columns=['mapa'], inplace=True)\n",
				"        data = df_trans_tipo.to_dict(orient='records')\n",
				"        filtro = df_trans['Unnamed: 2'] == 2900\n",
				"        df_trans_fitro = df_trans.loc[filtro]\n",
				"        coluna_remove = [\"Unnamed: 0\", \"Unnamed: 1\", \"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 5\", \"Unnamed: 6\",\"Unnamed: 8\", \"Unnamed: 9\", \"Unnamed: 11\",\"Unnamed: 13\", \"Unnamed: 14\", \"Unnamed: 15\", \"Unnamed: 16\", \"Unnamed: 17\"]\n",
				"        df_trans_fitro.drop(columns=coluna_remove, inplace=True)\n",
				"        df_trans_fitro['tipo'] = df_trans_fitro['Unnamed: 4'].apply(lambda x: extrair_tipo(x, data))\n",
				"        df_trans_fitro.rename(columns={\"Unnamed: 4\": \"conta_contabil_10\",\"Unnamed: 7\": \"desc_conta_contabil\",\"Unnamed: 10\": \"valor\",\"Unnamed: 12\": \"valor_1\"}, inplace=True)\n",
				"        dtype = {'conta_contabil_10':'int','desc_conta_contabil': 'str','valor': 'float','valor_1': 'float','tipo': 'str'}\n",
				"        df_trans_fitro.astype(dtype)\n",
				"\n",
				"    return df_trans_fitro\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"list_params = read_json_from_s3(par_config+\"config.json\")\n",
				"\n",
				"for params in list_params:\n",
				"    if params['active'] == True:\n",
				"        \n",
				"        file = params['file']\n",
				"        s3_path = params['s3_path']\n",
				"        match_id = params['match_id']\n",
				"        data = params['data']\n",
				"        if data == 'CURRENT_DATE':\n",
				"            current_date = datetime.now()\n",
				"            data = current_date.strftime('%Y-%m-%d')\n",
				"\n",
				"        intervalo = int(params['intervalo'])\n",
				"        catalog_name = params['catalog_name']\n",
				"        database_name = params['database_name']\n",
				"        table_name = params['table_name']\n",
				"        metodo = params['metodo']\n",
				"        sheet_name = params['sheet_name']\n",
				"        skiprows = int(params['skiprows'])\n",
				"        function = params['function']\n",
				"        snapshot = params['snapshot']\n",
				"        # Abrir o arquivo do S3\n",
				"        try:\n",
				"            file_schema_config = par_config+file.split('.')[0]+\".json\"\n",
				"            schema_map = read_json_from_s3(file_schema_config)\n",
				"        except:\n",
				"            schema_map = None\n",
				"            print(f\"Nao possui arquivo de mapeamento\")\n",
				"    \n",
				"        if metodo in [\"merge-full\",\"drop-insert\"]:\n",
				"            s3_path_file = s3_path+file\n",
				"            \n",
				"            if arquivo_existe_no_s3(s3_path_file):\n",
				"                with fs.open(s3_path_file, 'rb') as f:\n",
				"                    try:\n",
				"                        df_excel = pd.read_excel(f, sheet_name=sheet_name,skiprows=skiprows,engine='openpyxl')\n",
				"                    except:\n",
				"                        print(f\" O arquivo {file} fora de formato\")\n",
				"                        continue\n",
				"                        \n",
				"                    df = process_dataframe(df_excel,schema_map)\n",
				"                    retorno_processa_lake = processa_lake(metodo,snapshot,catalog_name,database_name,table_name,match_id,'',df)\n",
				"            else:\n",
				"                print(f\" O arquivo {file} nao existe no S3\")\n",
				"\n",
				"        elif metodo in [\"delete-insert-mes\"]:\n",
				"            resultado = gerar_lista_meses(data, intervalo)\n",
				"            for mes in resultado:\n",
				"                s3_path_file = s3_path+mes+'_'+file\n",
				"                print(s3_path_file)\n",
				"                \n",
				"                if arquivo_existe_no_s3(s3_path_file):\n",
				"                    with fs.open(s3_path_file, 'rb') as f:\n",
				"                        try:\n",
				"                            df_excel = pd.read_excel(f, sheet_name=sheet_name,skiprows=skiprows,engine='openpyxl')\n",
				"                        except:\n",
				"                            print(f\" O arquivo {file} fora de formato\")\n",
				"                            continue\n",
				"\n",
				"                        df_excel = transformacao_dataframe(df_excel,function)\n",
				"                        df = process_dataframe(df_excel,schema_map)\n",
				"                        df_excel['mes'] = mes\n",
				"                        retorno_processa_lake = processa_lake(metodo,snapshot,catalog_name,database_name,table_name,match_id,mes,df)\n",
				"                \n",
				"                else:\n",
				"                    print(f\" O arquivo {file} nao existe no S3\")\n",
				"        else:\n",
				"            print(f\"Metodo Inexistente\")\n",
				"\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"pygments_lexer": "python3",
			"version": "3.11.9"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
